
# 👽 6주차

**발표일**: 2025.07.04

---

# ⚙️ 주제
## 역전파  
- 발표자: **강한결, 조은비**  
- 참고 논문:  
[Learning representations by back-propagating errors](http://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf)

## 멀티모달 LMM (LLaVA)
- 발표자: **전민하, 최수빈**  
- 참고 논문:  
[Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485)

---

# 🗣️ 회고
## 강한결
### 받은 질문
- Q1: sigmoid 값이 0.8 이상이거나 0.2 미만인 경우만 정/오답으로 처리한다고 했는데, 그 사이의 값들이 나오면 어떻게 처리하는지?  
- A1: 실제로 모델을 설계한다면 다른 모델을 함께 활용하거나(앙상블), 오분류로 처리한 후 그 데이터를 바탕으로 미세조정하는 등의 방법이 있겠지만, 논문에서 나온 실험의 경우 이미 104개 케이스에 대해 모두 정/오답을 맞혔기 때문에 따로 처리할 일은 없었을 것.  
- Q2: 왜 임계치가 하필 0.2와 0.8일까?  
- A2: 논문에 명시적으로 설명되어 있는 부분은 없음. 그냥 일반적으로 적당한 값 넣어서 쓸 듯? 만약에 모든 정답 뉴런의 sigmoid 값이 0.9 이상이었으면 사후적으로 기준을 그렇게 바꾸지 않았을까 싶기도 하고..  
  
### 소감  
   : 부트캠프에서 역전파에 대해 처음 배울 때는 세부 알고리즘이나 수식을 뜯어 보지 않고 넘어가서 '저건 평생 이해하지 못하겠구나.'라는 식으로 생각했는데, 막상 논문을 읽어 보니 크게 어렵지 않고 chain rule과 편미분 기본 공식만 알고 있어도 충분히 이해할 수 있는 내용이어서 놀랍고 재밌었다. 빠르게 배우려다보면 많은 학습내용을 비유를 통해 어렴풋이 이해하고 넘어가게 되는데 논문을 통해 사실과 최대한 닮은 형태로 머릿속에 넣어두는 것이 도움이 되는 때가 있었으면 좋겠다.
   
---

## 조은비

### 받은 질문
- Q1: 가중치 감쇠를 통해 의미 없는 특성들을 없애는 것이, 모델의 리소스 대비 효율성만 올려주는 건지, 전체적인 성능 자체를 끌어올려주는 건지 궁금합니다.

### 소감  
   : Fig5의 아이디어를 낸 게 가장 신기했다. 역전파 알고리즘을 일방향 구조에 넣은 것도 신기한데, 여기서 더 나아가서 양방향 순환 네트워크에도 역전파를 적용시키기 위해서 시간 순을 넣어서 펼쳤다는 것 자체가, 어떻게 이런 아이디어를 처음으로 생각해낼 수 있었을까 싶어서 그 연구자들의 열의에 감탄했다

---

## 전민하  
### 받은 질문  
- Q1: zero-shot과 few-shot이 무엇인가요?
- A1: zero-shot은 예시 없이도 문제를 해결하는 방식이고, few-shot은 몇 개의 예시를 참고해서 문제를 해결하는 방식입니다. 모델이 학습 없이도 얼마나 일반화할 수 있는지를 평가할 때 사용됩니다.  
- Q2: 멀티턴의 목적이 무엇인가요?
- A2: **문맥 유지**입니다! 사용자와 여러 차례 대화를 주고받으며 이전 대화 내용을 기억하고, 이어지는 질문에 일관성 있고 정교하게 응답하기 위함입니다.
### 소감  
   : 이전 회차 생성형 ai에 이어서 멀티모달 lmm에 대해 알아보았다. 다른 느낌보다 그냥 이렇게까지 빠르게 많은 것들이 가능해졌다는 점 자체가 신기했다. 이게 된다고?를 거듭 반복하며 더욱 확장되고 있다. 지금도 이미 우리 실생활에 깊숙히 들어와 있고 없어서는 안 될 존재가 되어버린 게,,, 조금 무서울 정도다. 역전파 논문은 어렵다고만 생각했었는데 생각보다 수식이 간단해서 띠용했다. 특히 순환 구조를 시간순으로 펼치는 내용이 머리에 쏙쏙 박히게 설명해주셔서 인상 깊었다.  
   
----

## 최수빈
### 받은 질문  
   : 없음

### 소감  
   : 이전 회차 생성형 ai와 연계되는 LLaVA라는 모델을 다룬 논문을 읽게 되었다. 준비기간은 되게 길었지만 막상 닥치고나서야 준비하다보니 괴롭게 준비했던 기억이 난다. 이번에 준비할 땐 최대한 llm의 도움을 덜 받는 식으로 준비를 했었는데, 그렇기 때문에 다른 사람의 리뷰도 더 꼼꼼하게 읽어보고 번역본에 대해서도 더 자세하게 읽으면서 준비했던 것 같다. 그래서 여태껏 했던 것 중에서 폼이 제일 많이 들어 갔으며 최대한 논문 그 자체를 인용하면서 준비했다는 생각이 드는데, 이렇게 하다보니 내가 말하고 싶은 것들이 더 정리가 잘되고 많아지는 느낌? 그래서 그런지 다른 때와는 다르게 발표가 끝나니 아쉽다는 생각도 덜들었고, 더 의미있고 뿌듯했던 것 같다.
   논문에서 흥미로웠던 점은 모델이 생각보다 인식률이 좋다는 것이었다. 부록 부분에 머스크 사진을 보고 물어보는 부분이 있었는데, 일론 머스크의 얼굴을 합성한 라이온킹 이미지를 보여줘도 잘 인식한다는 점에서 데이터에 저장이 되어 있다면 매우 높은 인식을 확인할 수 있을 것이다. 또 반대로는 영어가 아닌 다른 문자(이치란 라멘)이나 제품 브랜드(요거트 사진)를 인식하지 못했는데, 머스크의 경우와 빗대본다면 다양한 유형의 지식을 학습만 시켜준다면 이것들도 똑같이 잘 인식하지 않을까란 생각도 들었고, 그런부분에서 범용성이 높은 모델이라는 생각이 들었다.
   또한 역전파도 재밌었는데, 가볍게 배울 때와는 다르게 심층적으로 파고들어서 얘기해주시는 모습 덕분에 더 쉽게 이해할 수 있었다.
   
